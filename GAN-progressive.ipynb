{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements Progressive GAN.\n",
    "The idea is to train at different resolutions\n",
    "\n",
    "<igm src=\"images/progressive.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  3521\n"
     ]
    }
   ],
   "source": [
    "manualSeed = np.random.randint(1, 10000) # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "if not os.path.isdir('visualization'):\n",
    "   os.mkdir('visualization')\n",
    "if not os.path.isdir('data'):\n",
    "   os.mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "conf = Config(im_shape = 4*2**n_layers,\n",
    "            latent_size = 100,\n",
    "            in_channels=3,\n",
    "            out_channels = 16,\n",
    "            gen_channels = 128,\n",
    "            n_layers = n_layers,\n",
    "            batch_size = 8,\n",
    "            num_iter = 10000,\n",
    "            num_workers=4,\n",
    "            device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset from image folder\n",
    "dataset = Dataset.ImageFolder(root=\"data/anime\",\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(conf.im_shape),\n",
    "                            transforms.CenterCrop(conf.im_shape),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "# loader that handle loading stuff\n",
    "loader = torch.utils.data.DataLoader(dataset, shuffle=True,#, pin_memory=True,\n",
    "            batch_size=conf.batch_size, \n",
    "            drop_last=True,\n",
    "            num_workers=conf.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True)\n",
    "                                  + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 64, 64]             432\n",
      "       BatchNorm2d-2           [-1, 16, 64, 64]              32\n",
      "         LeakyReLU-3           [-1, 16, 64, 64]               0\n",
      "            Conv2d-4           [-1, 32, 32, 32]           4,608\n",
      "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
      "         LeakyReLU-6           [-1, 32, 32, 32]               0\n",
      "      EncoderBlock-7           [-1, 32, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 16, 16]          18,432\n",
      "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
      "        LeakyReLU-10           [-1, 64, 16, 16]               0\n",
      "     EncoderBlock-11           [-1, 64, 16, 16]               0\n",
      "           Conv2d-12            [-1, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-13            [-1, 128, 8, 8]             256\n",
      "        LeakyReLU-14            [-1, 128, 8, 8]               0\n",
      "     EncoderBlock-15            [-1, 128, 8, 8]               0\n",
      "           Conv2d-16            [-1, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-17            [-1, 256, 4, 4]             512\n",
      "        LeakyReLU-18            [-1, 256, 4, 4]               0\n",
      "     EncoderBlock-19            [-1, 256, 4, 4]               0\n",
      "           Conv2d-20            [-1, 100, 1, 1]         409,600\n",
      "           Linear-21                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 802,805\n",
      "Trainable params: 802,805\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 3.38\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 6.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    '''\n",
    "    Typical CNN network that reduces spacial dimension and increase channels at each step.\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1, bias=False),\n",
    "                                nn.BatchNorm2d(out_channels),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return(self.block(x))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input = nn.Sequential(nn.Conv2d(config.in_channels, config.out_channels, 3, stride=1, padding=1, bias=False),\n",
    "                                nn.BatchNorm2d(config.out_channels),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                )\n",
    "        self.encode = nn.ModuleList([EncoderBlock(config.out_channels*2**i, config.out_channels*2**(i+1)) \n",
    "                                    for i in range(config.n_layers)])\n",
    "        \n",
    "        self.out =  nn.ModuleList([nn.Conv2d(config.out_channels*2**i, config.latent_size, 4, stride=1, padding=0, bias=False)\n",
    "                                    for i in range(config.n_layers+1)])       \n",
    "        self.shared_linear = nn.Linear(config.latent_size,1)\n",
    "        \n",
    "        self.n_layers = config.n_layers\n",
    "\n",
    "    def forward(self, x, step=None):\n",
    "        if step is None:\n",
    "            step = self.n_layers\n",
    "\n",
    "        x = self.input(x)\n",
    "        for i in range(step):\n",
    "            x = self.encode[i](x)\n",
    "        # output a single number for discriminator\n",
    "        x = self.out[step](x).squeeze()\n",
    "        x = self.shared_linear(x).mean()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "discriminator = Discriminator(conf).to(conf.device)\n",
    "summary(discriminator,[(3,4*2**conf.n_layers,4*2**conf.n_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 128, 4, 4]         204,800\n",
      "         PixelNorm-2            [-1, 128, 4, 4]               0\n",
      "         LeakyReLU-3            [-1, 128, 4, 4]               0\n",
      "   ConvTranspose2d-4            [-1, 128, 8, 8]         262,144\n",
      "         PixelNorm-5            [-1, 128, 8, 8]               0\n",
      "         LeakyReLU-6            [-1, 128, 8, 8]               0\n",
      "      DecoderBlock-7            [-1, 128, 8, 8]               0\n",
      "   ConvTranspose2d-8          [-1, 128, 16, 16]         262,144\n",
      "         PixelNorm-9          [-1, 128, 16, 16]               0\n",
      "        LeakyReLU-10          [-1, 128, 16, 16]               0\n",
      "     DecoderBlock-11          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-12          [-1, 128, 32, 32]         262,144\n",
      "        PixelNorm-13          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-14          [-1, 128, 32, 32]               0\n",
      "     DecoderBlock-15          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-16          [-1, 128, 64, 64]         262,144\n",
      "        PixelNorm-17          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-18          [-1, 128, 64, 64]               0\n",
      "     DecoderBlock-19          [-1, 128, 64, 64]               0\n",
      "           Conv2d-20            [-1, 3, 64, 64]             384\n",
      "================================================================\n",
      "Total params: 1,253,760\n",
      "Trainable params: 1,253,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 21.39\n",
      "Params size (MB): 4.78\n",
      "Estimated Total Size (MB): 26.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    '''\n",
    "    Reverse of Encoder\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                                    # nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "                                    # nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, bias=True),\n",
    "                                    # PixelNorm(),\n",
    "                                    # nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, bias=True),\n",
    "                                    nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                                    PixelNorm(),\n",
    "                                    nn.LeakyReLU(0.1),\n",
    "                                    )\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return(self.block(x))\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Sequential(nn.ConvTranspose2d(config.latent_size, config.gen_channels, 4, stride=1, bias=False),\n",
    "                                PixelNorm(),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                )\n",
    "                    \n",
    "        self.decode = nn.ModuleList([DecoderBlock(config.gen_channels, config.gen_channels) \n",
    "                                    for i in range(config.n_layers-1,-1,-1)])\n",
    "        \n",
    "        self.out = nn.ModuleList([nn.Conv2d(config.gen_channels, config.in_channels, 1, \n",
    "                                stride=1, padding=0, bias=False) for i in range(config.n_layers,-1,-1)])\n",
    "\n",
    "\n",
    "        self.out_channels = config.out_channels\n",
    "        self.n_layers = config.n_layers\n",
    "\n",
    "    def forward(self, x, step=None):\n",
    "        if step is None:\n",
    "            step = self.n_layers\n",
    "            \n",
    "        x = self.input(x)\n",
    "        for i in range(step):\n",
    "            x = self.decode[i](x)\n",
    "\n",
    "        x = self.out[step](x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "generator = Generator(conf).to(conf.device)\n",
    "summary(generator,(100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(x, netD, it):\n",
    "    '''\n",
    "    Display and save visualization\n",
    "    '''\n",
    "    netD.eval()\n",
    "    with torch.no_grad():\n",
    "        num_vis = min(conf.batch_size, 8)\n",
    "        x = x[0:num_vis]\n",
    "        x = x.cpu().numpy()\n",
    "        \n",
    "        for i, obj_plot in enumerate(x):\n",
    "            \n",
    "            obj_plot = np.moveaxis(obj_plot, 0, -1)\n",
    "            plt.imshow(obj_plot*0.5 +0.5)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            plt.savefig('visualization/'+ str(it)+'.jpg', format='jpg', bbox_inches='tight')\n",
    "    netD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = conf.device\n",
    "\n",
    "netD = Discriminator(conf).to(device)\n",
    "netG = Generator(conf).to(device)\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "\n",
    "fixed_noise = torch.randn(conf.batch_size, 100, 1, 1).to(device)\n",
    "\n",
    "loss_D = []\n",
    "loss_G = []\n",
    "loss_D_smooth = 0\n",
    "loss_G_smooth = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(conf.num_iter))\n",
    "data_iter = iter(loader)\n",
    "Diters = 2\n",
    "for it in pbar:\n",
    "    try:\n",
    "        x_real = data_iter.next()[0].to(device)\n",
    "    except(OSError, StopIteration):\n",
    "        data_iter = iter(loader)\n",
    "        x_real = data_iter.next()[0].to(device)\n",
    "    noise = torch.randn(conf.batch_size, 100, 1, 1).to(device)\n",
    "\n",
    "    for step in range(conf.n_layers+1):\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True # to avoid computation\n",
    "        for _ in range(Diters):\n",
    "            # train on multi resolution\n",
    "            scaled = F.interpolate(x_real, scale_factor=2**(step - conf.n_layers), mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "            netD.zero_grad()\n",
    "            errD_real = netD(scaled.detach(), step)\n",
    "            # noise that we will feed into gan.G\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # noise = torch.randn(conf.batch_size, 100, 1, 1).to(device)\n",
    "                x_fake = netG(noise.detach(), step)\n",
    "            errD_fake = netD(x_fake.detach(), step)\n",
    "            \n",
    "            # Earth Mover loss to udpate gan.D\n",
    "            errD = errD_real - errD_fake\n",
    "            errD.backward()\n",
    "\n",
    "            ### gradient penalty for D from the paper \"Improved Training of Wasserstein GANs\"\n",
    "            eps = torch.rand(conf.batch_size, 1, 1, 1).to(device)\n",
    "            x_hat = eps * scaled.data + (1 - eps) * x_fake.detach().data\n",
    "            x_hat.requires_grad = True\n",
    "            hat_predict = netD(x_hat, step=step)\n",
    "            grad_x_hat = grad(\n",
    "                outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
    "            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\n",
    "                                .norm(2, dim=1) - 1)**2).mean()\n",
    "            grad_penalty = 10 * grad_penalty\n",
    "            grad_penalty.backward()\n",
    "\n",
    "            optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "            \n",
    "        netG.zero_grad()\n",
    "\n",
    "        # noise = torch.randn(conf.batch_size, 100, 1, 1).to(device)\n",
    "        x_fake = netG(noise, step)\n",
    "        errG = netD(x_fake, step)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "\n",
    "    loss_D_smooth = 0.9*loss_D_smooth + 0.1*errD.item()\n",
    "    loss_D.append(loss_D_smooth)\n",
    "            \n",
    "    loss_G_smooth = 0.9*loss_G_smooth + 0.1*errG.item()\n",
    "    loss_G.append(loss_G_smooth)\n",
    "    \n",
    "    \n",
    "    pbar.set_description(f\"gan.D {loss_D_smooth:.5f} | gan.G {loss_G_smooth:.5f} | D_real {errD_real.item():.3f} | D_fake {errD_fake.item():.3f}\")\n",
    "    \n",
    "    if it % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            x_fake = netG(fixed_noise, conf.n_layers)\n",
    "        show_progress(x_fake, netD, it)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(range(len(loss_D)), loss_D)\n",
    "        plt.plot(range(len(loss_G)), loss_G)\n",
    "        plt.legend(['discriminator loss', 'generator loss'])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "655c588915bdf4d4fda03d550e181c0c6413d3619b1f10dd47e0fb3197f4a7b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ve': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
